{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.autograd import Variable\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./baseline_df.csv')\n",
    "print (df.shape)\n",
    "print(df.columns.values)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like Charlotte has done this already.\n",
    "df2 = df[df['land_mask']==1]\n",
    "\n",
    "# Drop these columns that don't matter.\n",
    "df2.drop(['pixel_id', 'land_mask'], axis=1, inplace=True)\n",
    "\n",
    "# We ignore pixels with no agriculture, so 0 or missing calories per ha.\n",
    "df2['calories_per_ha'] = df2['calories_per_ha'].replace({np.nan: 0})\n",
    "df2 = df2[df2['calories_per_ha'] != 0]\n",
    "\n",
    "# Replace all \"I don't know\" values with NaN, then nuke all rows with a NaN.\n",
    "df2['slope'] = df2['slope'].replace({0: np.nan})  # 143 NaN in 'slope' variable, and 0 means vertical...\n",
    "\n",
    "# 255 is the \"missing value\" value.\n",
    "for soil_var in ['workability_index', 'toxicity_index', 'rooting_conditions_index', 'oxygen_availability_index',\n",
    "                 'nutrient_retention_index', 'nutrient_availability_index', 'excess_salts_index', \n",
    "                 'protected_areas_index']:\n",
    "    df2[soil_var] = df2[soil_var].replace({255: np.nan})\n",
    "\n",
    "df2.drop(['calories_per_ha'], axis=1, inplace=True)\n",
    "    \n",
    "# Drop all rows with a NaN\n",
    "df2 = df2.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2['log_calories_per_ha'].describe())\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(df2['log_calories_per_ha'], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizes a column between -1 and 1.\n",
    "def normalize_col(df, col, xmin=None, xmax=None):\n",
    "    if not xmin: xmin = df[col].min()\n",
    "    if not xmax: xmax = df[col].max()\n",
    "    df[col] = 2 * ((df[col] - xmin) / (xmax - xmin)) - 1\n",
    "    # no return value, it's modified in place in df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in ['gdp_per_capita']:\n",
    "    df2[col] = df2[col].apply(lambda x: np.log(x) if x != 0 else 0)\n",
    "    df2.rename(columns = {col:'log_{}'.format(col)}, inplace = True)\n",
    "\n",
    "# Slope: 90 => 0 is horizontal. Slope is now in radians.\n",
    "df2['slope'] = df2['slope'].apply(lambda x:np.radians(-x))\n",
    "\n",
    "\n",
    "# We normalize all column except the *index ones.\n",
    "\n",
    "for col in df2:\n",
    "    if col=='log_calories_per_ha': continue\n",
    "    if '_index' in col: continue\n",
    "    normalize_col(df2, col)\n",
    "\n",
    "normalize_col(df2, 'log_calories_per_ha', 21.0, 24.0)  # Handcrafted.\n",
    "\n",
    "\n",
    "# Move log_calories_per_ha to the front.\n",
    "cols = df2.columns.tolist()\n",
    "cols.insert(0, cols.pop(cols.index('log_calories_per_ha')))\n",
    "df2 = df2.reindex(columns= cols)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in df2:\n",
    "    if 'index' in col: continue\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.distplot(df2[col], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2['log_calories_per_ha'].describe())\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(df2['log_calories_per_ha'], ax=ax)\n",
    "\n",
    "print(df2['slope'].describe())\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(df2['slope'], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.shape)\n",
    "print(df2.columns.values)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('./data_for_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get back some memory.\n",
    "del df\n",
    "del df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_for_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_exact_clusters(df):\n",
    "    clusters = dict()   # Each entry is key => [list of log_ca for records with this key]\n",
    "    for index, row in df.iterrows():\n",
    "        log_ca = row[0]   # Horrible HACK: log_calories_per_ha is the FIRST entry.\n",
    "        row2 = []\n",
    "        for cell in row[1:]:    # Horrible HACK: and the actual input are the last n-1 columns.\n",
    "            row2.append('{:.1f}'.format(cell))\n",
    "        key = '_'.join(row2)\n",
    "        if key not in clusters:\n",
    "            clusters[key] = [log_ca]\n",
    "        else:\n",
    "            clusters[key].append(log_ca)\n",
    "        \n",
    "    sorted_by_value = sorted(clusters.items(), key=lambda kv: len(kv[1]), reverse=True)\n",
    "    n = 0\n",
    "    for kv in sorted_by_value:\n",
    "        n += 1\n",
    "        if n>100: break\n",
    "        key = kv[0]\n",
    "        values = kv[1]\n",
    "        vmin, vmax, vave = min(values), max(values), sum(values)/len(values)\n",
    "        ratio = np.exp(vmax-vmin)  # max(cal)/min(cal)\n",
    "        print('{} \\n\\t min={:.3f} max={:.3f} ave={:.3f}, ratio={:.1f}, size={}\\n'.format(\n",
    "            key, min(values), max(values), sum(values)/len(values), ratio, len(values)))\n",
    "\n",
    "#     for val,ct in clusters.most_common()[:100]:\n",
    "#         print('{} clusters'.format(len(clusters)))\n",
    "#         print('{}\\t{}'.format(ct, val))\n",
    "\n",
    "find_exact_clusters(df)\n",
    "\n",
    "# Round to {:.0f}: 96383 clusters, top has 3216 entries\n",
    "# Round to {:.1f}: 477184 clusters, top has 96 entries\n",
    "# Round to {:.2f}: 579469 clusters, top has 2 entries\n",
    "# Round to {:.3f}: 579524 clusters, top has 1 entry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        #m.weight.data.fill_(0.01)\n",
    "        #m.bias.data.fill_(0.01)\n",
    "        m.weight.data.uniform_()\n",
    "        m.bias.data.uniform_()\n",
    "\n",
    "# Create a model with Sequential\n",
    "def create_model_2_layers(input_size, hidden_size, output_size, non_linearity = nn.Sigmoid()):\n",
    "    model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            non_linearity,\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            non_linearity,\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "            )\n",
    "    model.apply(init_weights)\n",
    "    return model\n",
    "\n",
    "def create_model_3_layers(input_size, h1, h2, h3, output_size, non_linearity = nn.Sigmoid()):\n",
    "    model = nn.Sequential(\n",
    "            nn.Linear(input_size, h1),\n",
    "            non_linearity,\n",
    "            nn.Linear(h1, h2),\n",
    "            non_linearity,\n",
    "            nn.Linear(h2, h3),\n",
    "            non_linearity,\n",
    "            nn.Linear(h3, output_size)\n",
    "            )\n",
    "    model.apply(init_weights)\n",
    "    return model\n",
    "\n",
    "def create_model_4_layers(input_size, h1, h2, h3, h4, output_size, non_linearity = nn.Sigmoid()):\n",
    "    model = nn.Sequential(\n",
    "            nn.Linear(input_size, h1),\n",
    "            non_linearity,\n",
    "            nn.Linear(h1, h2),\n",
    "            non_linearity,\n",
    "            nn.Linear(h2, h3),\n",
    "            non_linearity,\n",
    "            nn.Linear(h3, h4),\n",
    "            non_linearity,\n",
    "            nn.Linear(h4, output_size)\n",
    "            )\n",
    "    model.apply(init_weights)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, input_tensor, label_tensor, loss_function, optimizer):\n",
    "    # Forward propagation.\n",
    "    output = model(input_tensor)\n",
    "    # Compute the loss.\n",
    "    loss = loss_function(output, label_tensor)\n",
    "    # Zero the gradients.\n",
    "    model.zero_grad()\n",
    "    # Compute the gradients.\n",
    "    loss.backward()\n",
    "    # Adjust system along negative error gradient\n",
    "    optimizer.step()\n",
    "    return output, loss.item()\n",
    "\n",
    "\n",
    "def validation_step(model, input_tensor, label_tensor, loss_function):\n",
    "    output = model(input_tensor)\n",
    "    loss = loss_function(output, label_tensor)\n",
    "    return output, loss.item()\n",
    "\n",
    "\n",
    "def R2(y, y_hat):  # y are the labels, y_hat are the predictions.\n",
    "    y = np.array(y)\n",
    "    y_hat = np.array(y_hat)\n",
    "    SSE = (y-y_hat)*(y-y_hat)\n",
    "    y_mean = y.mean()\n",
    "    SST = (y-y_mean)*(y-y_mean)\n",
    "    return 1 - (SSE.sum() / SST.sum())\n",
    "\n",
    "\n",
    "def train_model(model, df, training_epochs = 10, optimizer=None):\n",
    "    loss_f = nn.MSELoss()\n",
    "    if not optimizer: optimizer = optim.Adam(model.parameters(), lr = 3e-4, weight_decay = 0.001)\n",
    "    \n",
    "    print_every = 10000\n",
    "    # Convert to tensors once. *****\n",
    "    nb_rows = df.shape[0]\n",
    "    nb_training_rows = nb_rows * 0.9\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        print(\"Epoch: {} - we reset the accumulated loss\".format(epoch+1))\n",
    "        loss_accum = 0\n",
    "        num_its = 0\n",
    "        valid_loss_accum = 0\n",
    "        valid_num_its = 0\n",
    "        ys = []    # Labels\n",
    "        preds = [] # Predictions\n",
    "        for index, row in df.iterrows():\n",
    "            row_as_list = row.tolist()\n",
    "            label = row_as_list[0]\n",
    "            label_tensor = torch.as_tensor([label])\n",
    "            all_other_columns = row_as_list[1:]\n",
    "            input_tensor = torch.tensor(all_other_columns)\n",
    "            \n",
    "            if index < nb_training_rows:    # Training the model.\n",
    "                # This only works because the label is the first column!!!\n",
    "                output, loss = training_step(model, input_tensor, label_tensor, loss_f, optimizer)\n",
    "                loss_accum += loss\n",
    "                num_its += 1\n",
    "                if num_its % print_every == 0 and num_its != 0:\n",
    "                    print(\"Loss (MSE): {:.3f}\".format(loss_accum / num_its))\n",
    "            else:                           # Validating the model.\n",
    "                output, loss = validation_step(model, input_tensor, label_tensor, loss_f)\n",
    "                valid_loss_accum += loss\n",
    "                valid_num_its += 1\n",
    "                pred = output.data.numpy()[0]\n",
    "                \n",
    "                ys.append(label)\n",
    "                preds.append(pred)\n",
    "#                 if valid_num_its % 100 == 0:\n",
    "#                     print('output={:.3f} label={:.3f} loss={:.3f}'.format(pred, label, loss))\n",
    "                if valid_num_its % print_every == 0 and valid_num_its != 0:\n",
    "                    print(\"Validation loss (MSE): {:.3f}\".format(valid_loss_accum / valid_num_its))\n",
    "                    print('R2={}'.format(R2(ys, preds)))\n",
    "                    \n",
    "                \n",
    "                \n",
    "    # Output model to file.\n",
    "    torch.save(model, 'model_pour_charlotte')\n",
    "\n",
    "# # Run the model\n",
    "# def run_model(model, input_tensor):\n",
    "#     return torch.nn.functional.softmax(model(input_tensor))\n",
    "\n",
    "# input_data = pd.read_csv('data/nutanix-test.tsv', sep='\\t')\n",
    "# probs = [None] * len(input_data)\n",
    "# for entry in test_data:\n",
    "#     probs[entry['src_line']] = float(run_model(ffnn, companyToTensor(entry))[1])\n",
    "# output_data = input_data.assign(Fitness=pd.Series(probs).values)\n",
    "# output_data = output_data.sort_values(by=['Fitness'], ascending=False)\n",
    "# output_data.to_csv('data/nutanix-test-classified.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = df.shape[1] - 1   # So far, 59 - 1 (label) = 58\n",
    "# 2 layers\n",
    "# model = create_model_2_layers(n_inputs, 5, 1)   # Validation loss (MSE): 0.051  R2=-0.0038\n",
    "model = create_model_2_layers(n_inputs, 20, 1)  # Loss (MSE): 0.051\n",
    "# model = create_model(n_inputs, 5, 1)   # MSE loss = 1.56 after 3 epochs.  (R2=0.173 after 3 epochs)\n",
    "# model = create_model(n_inputs, 10, 1)   # MSE loss = 1.56 after 3 epochs.\n",
    "# model = create_model(n_inputs, 20, 1)   # MSE loss = 1.55, val loss = 1.482 after 10 epochs\n",
    "# model = create_model(n_inputs, 50, 1)   # MSE loss = , val loss =  after 10 epochs\n",
    "\n",
    "# 3 layers\n",
    "# model = create_model(n_inputs, 5, 1)   # MSE loss = , val loss = , R2 =   after 10 epochs\n",
    "# model = create_model(n_inputs, 10, 1)   # MSE loss = 1.463, val loss = 1.448 after 10 epochs\n",
    "# model = create_model(n_inputs, 20, 1)   # MSE loss = 1.555, val loss = 1.616 after 2 epochs\n",
    "\n",
    "\n",
    "# All inputs normalized in -1..1, log_calories_per_ha, 2 layers\n",
    "# model = create_model_2_layers(n_inputs, 50, 1)  # MSE loss = 1.120, val loss = 1.072, R2=0.40 after 10 epochs\n",
    "# model = create_model_2_layers(n_inputs, 100, 1)   # MSE loss = 1.159, val loss = 1.087, R2=0.394 after 3 epochs\n",
    "# model = create_model_2_layers(n_inputs, 200, 1)   # MSE loss = , val loss = , R2= after  epochs\n",
    "\n",
    "# Every field included\n",
    "# model = create_model_2_layers(n_inputs, 50, 1)   # MSE loss = 1.123, val loss = 1.041, R2= 0.402 after 10 epochs\n",
    "# Removing lat/lon\n",
    "# model = create_model_2_layers(n_inputs, 50, 1)   # MSE loss = 1.176, val loss = 1.106, R2= 0.365 after 10 epochs\n",
    "# Removing (only) climatezone_*\n",
    "# model = create_model_2_layers(n_inputs, 50, 1)   # MSE loss = 1.147, val loss = 1.061, R2= 0.391 after 10 epochs\n",
    "# Removing log_min_to_market\n",
    "# model = create_model_2_layers(n_inputs, 50, 1)   # MSE loss = 1.134, val loss = 1.050, R2= 0.397 after 10 epochs\n",
    "# Removing gdp and log_gdp_per_capita\n",
    "# model = create_model_2_layers(n_inputs, 50, 1)   # MSE loss = 1.147, val loss = 1.065, R2= 0.384 after 10 epochs\n",
    "\n",
    "# All fields, *index have been encoded as 1-hot.\n",
    "# model = create_model_2_layers(n_inputs, 50, 1)   # MSE loss = 1.107, val loss = 1.108, R2= 0.406 after 10 epochs\n",
    "# Same, slope has been stretched.\n",
    "# model = create_model_2_layers(n_inputs, 50, 1)   # MSE loss = 1.125, val loss = 1.110, R2= 0.387 after 10 epochs\n",
    "\n",
    "# Back to regular slope (so back to normal), but we go nuts on width.\n",
    "# model = create_model_2_layers(n_inputs, 300, 1)\n",
    "                                                   # MSE loss = 1.165, val loss = 1.008, R2= 0.414 after 10 epochs\n",
    "\n",
    "# model = create_model_3_layers(n_inputs, 300, 100, 30, 1)\n",
    "                                                   # MSE loss = 1.131, val loss = 0.975, R2= 0.433 after 20 epochs\n",
    "    \n",
    "# model = create_model_3_layers(n_inputs, 10, 10, 10, 1)       # val=1.00, R2=0.415 after 6 epochs\n",
    "\n",
    "# model = create_model_4_layers(n_inputs, 10, 10, 10, 10, 1)     # val=0.989, R2=0.425, 15 epochs\n",
    "# model = create_model_4_layers(n_inputs, 20, 20, 20, 10, 1)     # val=0.970, R2=0.436, 20 epochs\n",
    "# model = create_model_4_layers(n_inputs, 50, 50, 20, 10, 1)     # val=0.973, R2=0.434, 20 epochs\n",
    "\n",
    "# model = create_model_4_layers(n_inputs, 10, 10, 10, 10, 1)     # val=0.989, R2=0.425, 100 epochs\n",
    "\n",
    "\n",
    "# model = create_model_4_layers(n_inputs, 20, 20, 20, 10, 1, non_linearity = nn.ReLU())     \n",
    "                                                                # val=0.910, R2=0.471, 100 epochs\n",
    "\n",
    "# model = create_model_4_layers(n_inputs, 50, 50, 50, 10, 1, non_linearity = nn.ReLU())     \n",
    "                                                                # val=1.012, R2=0.412, 7 epochs, weight_decay=0.01\n",
    "\n",
    "# model = create_model_4_layers(n_inputs, 50, 50, 50, 10, 1, non_linearity = nn.ReLU())     \n",
    "                                                                # val=1.121, R2=0.349, 88 epochs, weight_decay=0.1\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 3e-4, weight_decay = 0.1)\n",
    "train_model(model, df, training_epochs=100, optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df:\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.distplot(df[col], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
